
\documentclass{article}

\usepackage{amsmath,mathtools ,kotex , titling, tikz}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\LARGE#1\end{center}
    \vskip0.5em}%
}
\title{\textbf{The University of Yonsei
\\{\Large Faculty of Industrial Engineering}}}
\subtitle{Tsoding MachineLearning In C}
\author{\emph{Lim Dohyun}}

\begin{document}
\maketitle
\section{Introduction}
안녕하세요 한국말이 처음 입니다skddsds.

\section{Gradient Descent}

\begin{align}
    C'(w) = \lim_{\epsilon \to 0}\frac{C(w + \epsilon) -C(w)}{\epsilon}
\end{align}

\subsection{Twice}
\begin{verse}
    sequence of derivating C(w) with respect to w.
\end{verse}
\begin{align}
   C(w) &= \frac{1}{n}{\sum_{i=1}^{n}(x_iw - y_i)^2} \\
   C'(w) &= \left(\frac{1}{n}{\sum_{i=1}^{n}(x_iw - y_i)^2}\right)' \\
   &= \frac{1}{n}\left({\sum_{i=1}^{n}(x_iw - y_i)^2}\right)' \\
   &= \frac{2}{n}{\sum_{i=1}^{n}(x_iw - y_i)(x_i)'}
\end{align}

\begin{align}
    \shortintertext{Cost funtction}
    C(w) &= \frac{1}{n}{\sum_{i=1}^{n}(x_iw - y_i)^2} \\
    \shortintertext{Derviative of Cost function}
    C'(w) &= \frac{2}{n}{\sum_{i=1}^{n}(x_iw - y_i)(x_i)'}
\end{align} 

    \bigskip

\subsection{One Neuron Model with 2 inputs}

\def\d{1.5}

\begin{center}
    \begin{tikzpicture}
        \node (X) at (-\d,1) {$x$};
        \node (Y) at (-\d,-1) {$y$};
        \node[shape=circle,draw=black] (N) at (0,0) {$\sigma,b$ };
        \node (Z) at (\d,0) {$z$ };
        \path[->] (X) edge node[above] {$w_1$} (N);
        \path[->] (Y) edge node[above] {$w_2$} (N);
        \path[->] (N) edge (Z);
    \end{tikzpicture}
\end{center}

\begin{align}
    y &= \sigma(xw_1 + yw_2 + b) \\
    \sigma(x) &= \frac{1}{1 + e^{-x}} \\
    \sigma'(x) &= \sigma(x)(1 - \sigma(x)) 
\end{align}

\newpage

\subsubsection{Cost}

\def\pd[#1]{\partial_{#1}}
\def\avgsum[#1,#2]{\frac{1}{#2}\sum_{#1=1}^{#2}}

subscript i is reffering to a sample number
\begin{align}
    a_i &= \sigma(x_iw_1 + yw_2 +b) \\
    \pd[w_1]a_i 
        &= \pd[w_1](\sigma(x_iw_1 + yw_2 + b)) \\
        &= a_i(1 - a_i)\pd[w_1](x_iw_1 + yw_2 + b) \\
        &= a_i(1 - a_i)x_i \\
    \pd[w_2]a_i
        &= a_i(1 - a_i)y_i \\
    \pd[b]a_i
        &= a_i(1 - a_i) \\
    C &= \avgsum[i,n](a_i - y_i)^2 \\
    \pd[w_1]C 
        &= \avgsum[i, n]\pd[w_1]\left((a_i - z_i)^2\right) \\
        &= \avgsum[i, n]2(a_i - z_i)\pd[w_1]a_i \\
        &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)x_i \\
    \pd[w_2]C
        &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)y_i \\
    \pd[b]C
        &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)
\end{align}

\subsection{Two Neuron Model with 1 inputs}

\def\d{2.0}
\def\s[#1]{^{(#1)}}
\begin{center}
    \begin{tikzpicture}
        \node (X) at (-\d,0) {$x$};
        \node[shape=circle,draw=black] (N1) at (0,0) {$\sigma,b\s[1]$ };
        \node[shape=circle,draw=black] (N2) at (\d,0) {$\sigma,b\s[2]$ };
        \node (Y) at (2*\d,0) {$y$ };
        \path[->] (X) edge node[above] {$w\s[1]$} (N1);
        \path[->] (N1) edge node[above] {$w\s[2]$} (N2); 
        \path[->] (N2) edge (Y);
    \end{tikzpicture}
\end{center}

superscript i is reffering to a layer number
\begin{align}
    a_i\s[1] &= \sigma(x_iw\s[1] + b\s[1]) \\
    \pd[{w\s[1]}]a_i\s[1] &= a_i\s[1](1 - a_i\s[1])x_i \\
    \pd[{b\s[1]}]a_i\s[1] &= a_i\s[1](1 - a_i\s[1]) \\
    a_i\s[2] &= \sigma(a_i\s[1]w\s[2] + b\s[2]) \\
    \pd[{w\s[2]}]a_i\s[2] &= a_i\s[2](1 - a_i\s[2])a_i\s[1] \\
    \pd[{b\s[2]}]a_i\s[2] &= a_i\s[2](1 - a_i\s[2]) \\
    \pd[{a_i\s[1]}]a_i\s[2] &= a_i\s[2](1 - a_i\s[2])w\s[2] \\
    C\s[2] &= \avgsum[i, n] (a_i\s[2] - y_i)^2 \\
    \pd[{w\s[2]}] C\s[2]
        &= \avgsum[i, n] \pd[{w\s[2]}]((a_i\s[2] - y_i)^2) \\
        &= \avgsum[i, n] 2(a_i\s[2] - y_i)\pd[{w\s[2]}](a_i\s[2]) \\
        &= \avgsum[i, n] 2(a_i\s[2] - y_i)a_i\s[2](1 - a_i\s[2])a_i\s[1] \\
    \pd[{b\s[2]}] C\s[2]
        &= \avgsum[i, n] 2(a_i\s[2] - y_i)a_i\s[2](1 - a_i\s[2]) \\
    \pd[{a_i\s[1]}] C\s[2]
        &= \avgsum[i, n] 2(a_i\s[2] - y_i)a_i\s[2](1 - a_i\s[2])w\s[2] \\
    e_i
        &= a_i\s[1] - \pd[{a_i\s[1]}]C\s[2] \\
    C\s[1]
        &= \avgsum[i, n] (a_i\s[1] - e_i)^2 \\
    \pd[{w\s[1]}]C\s[1]
        &= \pd[{w\s[1]}]\left(\avgsum[i, n] (a_i\s[1] - e_i)^2\right) \\
        &= \avgsum[i, n] \pd[{w\s[1]}]\left((a_i\s[1] - e_i)^2\right) \\
        &= \avgsum[i, n] 2(a_i\s[1] - e_i) \pd[{w\s[1]}]a_i\s[1] \\
        &= \avgsum[i, n] 2(a_i\s[1] -(a_i\s[1] - \pd[{a_i\s[1]}]C\s[2])) \pd[{w\s[1]}]a_i\s[1] \\
        &= \avgsum[i, n] 2(\pd[{a_i\s[1]}]C\s[2]) \pd[{w\s[1]}]a_i\s[1] \\
        &= \avgsum[i, n] 2(\pd[{a_i\s[1]}]C\s[2]) a_i\s[1](1 - a_i\s[1])x_i \\
    \pd[{b\s[1]}]C\s[1]
        &= \avgsum[i, n] 2(\pd[{a_i\s[1]}]C\s[2]) \pd[{b\s[1]}]a_i\s[1] \\
        &= \avgsum[i, n] 2(\pd[{a_i\s[1]}]C\s[2]) a_i\s[1](1 - a_i\s[1])
\end{align}

\subsection{Arbitrary Neurons Model with 1 inputs}

Let's assume that we have $m$ layers.

\subsubsection{Feed-Forward}

Let's assume that $a_i\s[0]$ is $x_i$

\begin{align}
    a_i\s[l] &= \sigma(a_i\s[l-1]w\s[l] + b\s[l]) \\
    \pd[{w\s[l]}]a_i\s[l] &= a_i\s[l](1 - a_i\s[l])a_i\s[l-1] \\
    \pd[{b\s[l]}]a_i\s[l] &= a_i\s[l](1 - a_i\s[l]) \\
    \pd[{a_i\s[l-1]}]a_i\s[l] &= a_i\s[l](1 - a_i\s[l])w\s[l] \\
\end{align}

\subsubsection{Back-Propagation - Tsoding's apporach}

Let's denote $a_i\s[m] - y_i$ as $\pd[{a_i\s[m]}]C\s[m+1]$

\begin{align}
    C\s[l]
        &= \avgsum[i, n] (\pd[{a_i\s[l]}]C\s[l+1])^2 \\
    \pd[{w\s[l]}]C\s[l]
        &= \avgsum[i, n] 2(\pd[{a_i\s[l]}]C\s[l+1]) a_i\s[l](1 - a_i\s[l])a_i\s[l-1] \\
    \pd[{b\s[l]}]C\s[l]
        &= \avgsum[i, n] 2(\pd[{a_i\s[l]}]C\s[l+1]) a_i\s[l](1 - a_i\s[l]) \\
    \pd[{a_i\s[l-1]}] C\s[l]
        &= \avgsum[i, n] 2(\pd[{a_i\s[l]}]C\s[l+1])a_i\s[l](1 - a_i\s[l])w\s[l]
\end{align}

\subsubsection{Back-Propagation - Traditional apporach}

above we denoted $a_i\s[m] - y_i$ as $\pd[{a_i\s[m]}]C\s[m+1]$ but here we are just going to use $a_i\s[m] - y_i$

\begin{align}
    C\s[l]
        &= (a_i\s[m] - y)^2\\
    \pd[{a\s[m]}]C\s[l]
        &= 2 \cdot (a_i\s[m] - y)\\
    \pd[{w\s[l]}]C\s[l]
        &= \pd[{a_i\s[l]}]C\s[l+1] \cdot a_i\s[l](1 - a_i\s[l])a\s[l-1] \\
    \pd[{b\s[l]}]C\s[l]
        &= \pd[{a_i\s[l]}]C\s[l+1] \cdot a_i\s[l](1 - a_i\s[l]) \\
    \pd[{a_i\s[l-1]}] C\s[l]
        &= \pd[{a_i\s[l]}]C\s[l+1] \cdot a_i\s[l](1 - a_i\s[l-1])w\s[l] \\
\end{align}


\end{document}